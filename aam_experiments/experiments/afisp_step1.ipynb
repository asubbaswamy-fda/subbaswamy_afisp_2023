{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import modin.pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.weightstats import ztest, ttest_ind\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science', 'nature', 'no-latex'])\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from plotnine import *\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../../src')\n",
    "sys.path.append('../../slicefinder')\n",
    "from slice_finder import SliceFinder\n",
    "from clustering_analysis import ClusteringEstimator\n",
    "from stability_analysis import LatentSubgroupShiftEstimator\n",
    "import utils\n",
    "import sirus\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_hdf('/data/adarsh/fda_project_data/raw_complete_dataset.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_df = pd.read_hdf('/data/adarsh/fda_project_data/hcgh_combined_test_df_7_11_2022.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commorbidity_df = pd.read_csv(\"/data/adarsh/fda_project_data/study_cohort_commorbidity_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commorbidity_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge relevant dataframes\n",
    "wc_test_df = (scaled_test_df\n",
    "              .merge(raw_df[['enc_id', 'obs_time', 'age']], on=['enc_id', 'obs_time'], how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = scaled_test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_test_df = wc_test_df.merge(commorbidity_df, on ='enc_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at shift in demographics, commorbidities, and circumstances of admission\n",
    "features = ['age', 'sex', 'ed_admit', 'season_1', 'season_2', 'season_3',\n",
    "            'trauma_level', 'hospital_size_large'] + list(commorbidity_df.columns)[1:]\n",
    "\n",
    "continuous_features = ['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make age numeric\n",
    "wc_test_df['age'] = wc_test_df.age.replace({'>= 90':90}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = wc_test_df[features]\n",
    "orig_X = np.copy(X_test)\n",
    "y_test = wc_test_df['label']\n",
    "\n",
    "all_features = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([wc_test_df.drop(features, axis=1), X_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFISP Step 1: Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = full_df.label.values\n",
    "test_preds = full_df['aam_prediction'].values \n",
    "hinge_auc_loss = utils.torch_roc_auc_surrogate(y_test, test_preds, 'hinge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = hinge_auc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_feature_data = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stability_analysis = LatentSubgroupShiftEstimator(cv=5, \n",
    "                                                  verbose=True, \n",
    "                                                  eps=1e-5, \n",
    "                                                  subset_fractions=np.arange(0.05, 1, 0.05)\n",
    "                                                 )\n",
    "sa_risks = stability_analysis.fit(subgroup_feature_data, hinge_auc_loss, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_masks = stability_analysis.subset_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = stability_analysis.check_subset_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that loss increases as worst-performing subset size gets smaller\n",
    "plt.plot(stability_analysis.subset_fractions, sa_risks, '.-', label='AAM')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpop_aucs = []\n",
    "news_aucs = []\n",
    "for m in sa_masks:\n",
    "    subpop_aucs.append(roc_auc_score(y_test[m], test_preds[m]))\n",
    "    news_aucs.append(roc_auc_score(y_test[m], full_df.baseline_prediction.values[m]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also compute bootstrap confidence intervals\n",
    "bootstrap_cis = np.zeros((len(sa_masks), 2))\n",
    "for i, alpha in tqdm(enumerate(stability_analysis.subset_fractions)):\n",
    "    mask = sa_masks[i]\n",
    "    mean, upper, lower = utils.bootstrap_ci(y_test[mask], test_preds[mask])\n",
    "    bootstrap_cis[i, 0] = lower\n",
    "    bootstrap_cis[i, 1] = upper\n",
    "\n",
    "plt.plot(stability_analysis.subset_fractions, subpop_aucs)\n",
    "plt.fill_between(stability_analysis.subset_fractions,\n",
    "                 bootstrap_cis[:, 0], \n",
    "                 bootstrap_cis[:, 1], \n",
    "                 alpha=0.25, \n",
    "                 label='Bootstrap')\n",
    "plt.xlabel('Subset Fraction')\n",
    "plt.ylabel('Brier Score')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_curve_df = pd.DataFrame(\n",
    "    {\n",
    "        'Subset Fraction': stability_analysis.subset_fractions,\n",
    "        'AUROC': subpop_aucs, \n",
    "        'Lower': bootstrap_cis[:, 0],\n",
    "        'Upper': bootstrap_cis[:, 1],\n",
    "        'Name': 'AAM'\n",
    "    })\n",
    "baseline_curve_df = pd.DataFrame(\n",
    "    {\n",
    "        'Subset Fraction': stability_analysis.subset_fractions,\n",
    "        'AUROC': news_aucs[-1], \n",
    "        'Name': 'Baseline'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stability_analysis.subset_fractions, subpop_aucs, '.-', label='AAM')\n",
    "# plt.fill_between(stability_analysis.subset_fractions, roc_ci[:, 0], roc_ci[:, 1], alpha=0.25)\n",
    "\n",
    "# plt.plot(alphas, news_aucs, label='NEWS (Baseline)')\n",
    "plt.plot(stability_analysis.subset_fractions, \n",
    "         np.ones_like(news_aucs) * news_aucs[-1], \n",
    "         '--', \n",
    "         label='Baseline Full Performance')\n",
    "plt.fill_between(stability_analysis.subset_fractions,\n",
    "                 bootstrap_cis[:, 0], \n",
    "                 bootstrap_cis[:, 1], \n",
    "                 alpha=0.25)\n",
    "plt.ylabel('AUROC')\n",
    "plt.xlabel('Subset Fraction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0, 1.05)\n",
    "# plt.grid()\n",
    "plt.savefig('figs/stability_curve.pdf', dpi=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = []\n",
    "p_vals = []\n",
    "\n",
    "for i, a in enumerate(stability_analysis.subset_fractions):\n",
    "    idxs = sa_masks[i]\n",
    "    odxs = ~sa_masks[i]\n",
    "    cds.append(sirus.cohens_d(test_loss[idxs], test_loss[odxs]))\n",
    "    pval = ttest_ind(test_loss[idxs], \n",
    "                         x2=test_loss[odxs], \n",
    "                         value=0.,\n",
    "                         alternative='larger',\n",
    "                         usevar='unequal')[1]\n",
    "    p_vals.append(pval)\n",
    "        \n",
    "    \n",
    "plt.plot(stability_analysis.subset_fractions, cds)\n",
    "plt.xlabel('Subset Fraction')\n",
    "plt.ylabel('Cohen\\'s d (Effect Size)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind, max_cd = sirus.find_max_effect_size(sa_masks, test_loss)\n",
    "print(max_ind, stability_analysis.subset_fractions[max_ind], max_cd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ind = np.where(np.array(subpop_aucs) < news_aucs[-1])[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFISP Step 2: Subgroup Phenotype Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_df = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_df['trauma_level'] = phenotype_df['trauma_level'].astype(int).astype(\"category\")\n",
    "phenotype_df['admit_source'] = phenotype_df['admit_source'].astype(int).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_df = pd.get_dummies(phenotype_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_df['subset_label'] = sa_masks[max_ind]*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_df.to_csv(\"sirus_files/aam_for_sirus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 3\n",
    "rule_max = 50\n",
    "sirus_rules_fname = f\"tmp/afisp_sirus_rules_{rule_max}_rules_{depth}_depth.txt\"\n",
    "\n",
    "subprocess.call((f\"/home/adarsh.subbaswamy/anaconda3/envs/afisp/bin/Rscript\" \n",
    "f\" run_sirus.r\" \n",
    "f\" --input {df_fname} \"\n",
    "f\" --output {sirus_rules_fname}\"\n",
    "f\" --depth {depth}\"\n",
    "f\" --rule.max {rule_max}\"\n",
    "f\" --cv\"),\n",
    "shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sirus_rules = sirus.get_sirus_rules(sirus_rules_fname)\n",
    "rule_p_values = sirus.precompute_p_values(sirus_rules, phenotype_df, test_loss)\n",
    "significant_rules = sirus.holm_bonferroni_correction(rule_p_values)\n",
    "\n",
    "extracted_rules = sirus.effect_size_filtering(significant_rules, phenotype_df, test_loss, \n",
    "                                                  effect_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_aucs = []\n",
    "r_ls = []\n",
    "r_us = []\n",
    "ns = []\n",
    "\n",
    "for rule in tqdm(extracted_rules):\n",
    "    rows = phenotype_df.eval(str(rule))\n",
    "    m, l, u = utils.bootstrap_ci(y_test[rows], test_preds[rows])\n",
    "    ns.append(np.sum(rows))\n",
    "\n",
    "    r_aucs.append(m)\n",
    "    r_ls.append(l)\n",
    "    r_us.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aam_sirus_df = pd.DataFrame(\n",
    "    {\n",
    "        'Phenotype': extracted_rules, \n",
    "        'AUROC': r_aucs, \n",
    "        'N': ns, \n",
    "        'Lower': r_ls, \n",
    "        'Upper': r_us\n",
    "    }).sort_values(by='AUROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aam_sirus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at prevalence of AFISP subgroups in each worst-case subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prevalence of each subgroup in worst-case subsets\n",
    "prevalences = [[] for _ in range(len(aam_sirus_df))]\n",
    "subgroup_num = [[i + 1] * len(sa_masks) for i in range(len(aam_sirus_df))]\n",
    "\n",
    "for m in tqdm(sa_masks):\n",
    "    for i in range(len(aam_sirus_df)):\n",
    "        prev = phenotype_df[m].eval(aam_sirus_df.Phenotype.values[i]).mean()\n",
    "        prevalences[i].append(prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('rainbow', len(prevalences))\n",
    "for i in range(len(prevalences)):\n",
    "    plt.plot(stability_analysis.subset_fractions, prevalences[i], label=f'Subgroup {i+1}', color=cmap(i))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Subset Fraction')\n",
    "plt.ylabel('Prevalence in Subset')\n",
    "plt.savefig('figs/prevalences.pdf', dpi=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SliceFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sfX = X_test.copy()\n",
    "sf = SliceFinder(None, (sfX, pd.DataFrame({'y': y_test})))\n",
    "d1_slices = sf.slicing()\n",
    "d2_slices = sf.crossing(d1_slices, 2)\n",
    "candidate_slices = d1_slices + d2_slices\n",
    "print(\"Slices acquired\")\n",
    "# candidate_rules = [sirus.slice_to_equality_rule(s) for s in candidate_slices]\n",
    "\n",
    "# rule_p_values = sirus.precompute_p_values(candidate_rules, sfX, test_loss)\n",
    "# significant_rules = sirus.holm_bonferroni_correction(rule_p_values)\n",
    "\n",
    "# sf_extracted_rules = sirus.effect_size_filtering(significant_rules, sfX, test_loss, \n",
    "#                                               effect_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "candidate_rules = [sirus.slice_to_equality_rule(s) for s in candidate_slices]\n",
    "\n",
    "rule_p_values = sirus.precompute_p_values(candidate_rules, sfX, test_loss)\n",
    "significant_rules = sirus.holm_bonferroni_correction(rule_p_values)\n",
    "\n",
    "sf_extracted_rules = sirus.effect_size_filtering(significant_rules, sfX, test_loss, \n",
    "                                              effect_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sf_extracted_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_rs = []\n",
    "r_aucs = []\n",
    "r_ls = []\n",
    "r_us = []\n",
    "ns = []\n",
    "\n",
    "for rule in tqdm(sf_extracted_rules):\n",
    "    rows = sfX.eval(str(rule))\n",
    "    \n",
    "    if all(x==1 for x in y_test[rows]) or all(x==0 for x in y_test[rows]):\n",
    "        continue\n",
    "\n",
    "    m, l, u = utils.bootstrap_ci(y_test[rows], test_preds[rows])\n",
    "    ns.append(np.sum(rows))\n",
    "\n",
    "    r_aucs.append(m)\n",
    "    r_ls.append(l)\n",
    "    r_us.append(u)\n",
    "    r_rs.append(rule)\n",
    "    \n",
    "sf_sirus_df = pd.DataFrame(\n",
    "    {\n",
    "        'Phenotype': r_rs, \n",
    "        'AUROC': r_aucs, \n",
    "        'N': ns, \n",
    "        'Lower': r_ls, \n",
    "        'Upper': r_us\n",
    "    }).sort_values(by='AUROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_sirus_df.query('N >= 400')# .iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = ClusteringEstimator(verbose=True)\n",
    "cl.fit(subgroup_feature_data, hinge_auc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_df = X_test.copy()\n",
    "phenotype_df['trauma_level'] = phenotype_df['trauma_level'].astype(int).astype(\"category\")\n",
    "phenotype_df['admit_source'] = phenotype_df['admit_source'].astype(int).astype(\"category\")\n",
    "phenotype_df = pd.get_dummies(phenotype_df)\n",
    "phenotype_df['subset_label'] = cl.masks_*1\n",
    "phenotype_df.to_csv(\"sirus_files/clustering_for_sirus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_rules_fname = \"sirus_files/clustering_rules_d3.txt\"\n",
    "clustering_rules = sirus.get_sirus_rules(clustering_rules_fname)\n",
    "clustering_rule_p_values = sirus.precompute_p_values(clustering_rules, phenotype_df, test_loss)\n",
    "clustering_significant_rules = sirus.holm_bonferroni_correction(clustering_rule_p_values)\n",
    "\n",
    "clustering_extracted_rules = sirus.effect_size_filtering(clustering_significant_rules, phenotype_df, test_loss, \n",
    "                                                  effect_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_significant_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_rs = []\n",
    "r_aucs = []\n",
    "r_ls = []\n",
    "r_us = []\n",
    "ns = []\n",
    "\n",
    "for rule in tqdm([x[0] for x in clustering_significant_rules]):\n",
    "    rows = sfX.eval(str(rule))\n",
    "    \n",
    "    if all(x==1 for x in y_test[rows]) or all(x==0 for x in y_test[rows]):\n",
    "        continue\n",
    "\n",
    "    m, l, u = utils.bootstrap_ci(y_test[rows], test_preds[rows])\n",
    "    ns.append(np.sum(rows))\n",
    "\n",
    "    r_aucs.append(m)\n",
    "    r_ls.append(l)\n",
    "    r_us.append(u)\n",
    "    r_rs.append(rule)\n",
    "    \n",
    "clustering_sirus_df = pd.DataFrame(\n",
    "    {\n",
    "        'Phenotype': r_rs, \n",
    "        'AUROC': r_aucs, \n",
    "        'N': ns, \n",
    "        'Lower': r_ls, \n",
    "        'Upper': r_us\n",
    "    }).sort_values(by='AUROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_filtered_df = sf_sirus_df.query(\"N >= 400\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare groups found by SF and AFISP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_slices = np.random.choice(candidate_rules, replace=False, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_rs = []\n",
    "r_aucs = []\n",
    "r_ls = []\n",
    "r_us = []\n",
    "ns = []\n",
    "\n",
    "for rule in tqdm(random_slices):\n",
    "    rows = sfX.eval(str(rule))\n",
    "    \n",
    "    if all(x==1 for x in y_test[rows]) or all(x==0 for x in y_test[rows]):\n",
    "        continue\n",
    "\n",
    "    m, l, u = utils.bootstrap_ci(y_test[rows], test_preds[rows])\n",
    "    ns.append(np.sum(rows))\n",
    "\n",
    "    r_aucs.append(m)\n",
    "    r_ls.append(l)\n",
    "    r_us.append(u)\n",
    "    r_rs.append(rule)\n",
    "    \n",
    "random_slice_df = pd.DataFrame(\n",
    "    {\n",
    "        'Phenotype': r_rs, \n",
    "        'AUROC': r_aucs, \n",
    "        'N': ns, \n",
    "        'Lower': r_ls, \n",
    "        'Upper': r_us\n",
    "    }).sort_values(by='AUROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_slice_matrix = np.zeros((len(X_test), len(sf_filtered_df) + len(aam_sirus_df) + len(random_slice_df)))\n",
    "\n",
    "for j, r in tqdm(enumerate(sf_filtered_df.Phenotype.values)):\n",
    "    indicators = sfX.eval(r) * 1\n",
    "    full_slice_matrix[:, j] = indicators\n",
    "    \n",
    "for j, r in tqdm(enumerate(aam_sirus_df.Phenotype.values)):\n",
    "    indicators = phenotype_df.eval(r) * 1\n",
    "    full_slice_matrix[:, len(sf_filtered_df) + j] = indicators\n",
    "    \n",
    "for j, r in tqdm(enumerate(random_slice_df.Phenotype.values)):\n",
    "    indicators = sfX.eval(r) * 1\n",
    "    full_slice_matrix[:, len(sf_filtered_df) + len(aam_sirus_df) + j] = indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# perform PLS; use CV to pick num components\n",
    "plsr = PLSRegression()\n",
    "cv = GridSearchCV(plsr, cv=5, param_grid = {\n",
    "    'n_components':list(range(2, 20))\n",
    "}, verbose=5, n_jobs=32, scoring=\"neg_mean_squared_error\")\n",
    "cv.fit(full_slice_matrix, utils.cross_entropy(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_estimator_.n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plsr = PLSRegression(n_components=cv.best_estimator_.n_components)\n",
    "plsr.fit(full_slice_matrix, test_loss)\n",
    "plsr_X, plsr_Y = plsr.transform(full_slice_matrix, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = plsr.x_loadings_ # pca.components_.T\n",
    "plt.scatter(plsr.x_loadings_[:len(sf_filtered_df), 0], \n",
    "            plsr.x_loadings_[:len(sf_filtered_df), 1], \n",
    "            label='SliceFinder Slices', color='purple')\n",
    "plt.scatter(plsr.x_loadings_[len(sf_filtered_df) + len(aam_sirus_df):, 0], \n",
    "            plsr.x_loadings_[len(sf_filtered_df) + len(aam_sirus_df):, 1], \n",
    "            label='Random Slices', color='orange')\n",
    "\n",
    "plt.xlabel(\"Partial Least Squares Dimension 1\")\n",
    "plt.ylabel(\"Partial Least Squares Dimension 2\")\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.box(False)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('figs/pls_random_vs_sf_nature_no_afisp.pdf', dpi=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings = plsr.x_loadings_ # pca.components_.T\n",
    "plt.scatter(plsr.x_loadings_[:len(sf_filtered_df), 0], \n",
    "            plsr.x_loadings_[:len(sf_filtered_df), 1], \n",
    "            c=sf_filtered_df['AUROC'], cmap='viridis')\n",
    "\n",
    "for i in range(len(aam_sirus_df)):\n",
    "    \n",
    "    x = pca_embeddings[i+len(sf_filtered_df), 0] - 0.007\n",
    "    y = pca_embeddings[i+len(sf_filtered_df), 1]\n",
    "    \n",
    "    if i == 4 or i == 0:\n",
    "        y -= 0.0025\n",
    "    if i == 6:\n",
    "        y += 0.0025\n",
    "    if i == 2 or i == 8:\n",
    "        y += 0.001\n",
    "    if i == 9:\n",
    "        y -= 0.005\n",
    "\n",
    "    \n",
    "    plt.annotate(\"\", xy=pca_embeddings[i + len(sf_filtered_df)], xytext=(0, 0),\n",
    "            arrowprops=dict(arrowstyle=\"->\",color='k', linewidth=1))\n",
    "    plt.text(x, \n",
    "             y,\n",
    "             f\"AFISP {i+1}\", size=6\n",
    "            )\n",
    "#     plt.arrow(x=0, y=0, \n",
    "#               dx=pca_embeddings[i + len(sf_filtered_df), 0], \n",
    "#               dy=pca_embeddings[i + len(sf_filtered_df), 1], color='k')\n",
    "    \n",
    "# plt.scatter(pca_embeddings[-len(aam_sirus_df):, 0], \n",
    "#             pca_embeddings[-len(aam_sirus_df):, 1], \n",
    "#             label=\"AFISP rules\")\n",
    "\n",
    "plt.xlabel(\"Partial Least Squares Dimension 1\")\n",
    "plt.ylabel(\"Partial Least Squares Dimension 2\")\n",
    "plt.xlim(-0.06, 0.0025)\n",
    "plt.colorbar(label='AUROC')\n",
    "# plt.legend()\n",
    "plt.savefig('figs/pls_auroc.pdf', dpi=240)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
